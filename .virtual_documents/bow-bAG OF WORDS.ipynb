import pandas as pd
messages=pd.read_csv('spam.csv',encoding='latin-1', usecols=[0,1])
messages = messages[['v1','v2']].rename(columns={'v1':'label','v2':'message'})
messages


## Data Cleaning and PreProcessing
import re
import nltk
nltk.download('stopwords')


from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer

ps = PorterStemmer()
ss = SnowballStemmer('english')
lemmatizer = WordNetLemmatizer()


corpus= []
for i in range(0,len(messages)):
    review=re.sub('[^a-zA-Z]',' ', messages['message'][i])
    review=review.lower()
    review=review.split()
    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)


corpus


## Create the bag of words

from sklearn.feature_extraction.text import CountVectorizer
## for binary bag of words enable binary to true
cv=CountVectorizer(max_features=2500, binary=True)


X=cv.fit_transform(corpus).toarray()


X.shape


cv.vocabulary_





from sklearn.feature_extraction.text import CountVectorizer

cv=CountVectorizer(max_features=2500, binary=True,ngram_range=(1,2))


Z = cv.fit_transform(corpus).toarray()
Z


cv.vocabulary_



