import pandas as pd

messages = pd.read_csv('spam.csv',encoding='latin-1',usecols=[0,1])

messages = messages.rename(columns={'v1':'label','v2':'message'})
messages


import re
import nltk
nltk.download('stopwords')


from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,SnowballStemmer,WordNetLemmatizer

ps = PorterStemmer()
ss = SnowballStemmer('english')
lemmatizer = WordNetLemmatizer()


corpus_ps = []

for i in range(0,len(messages)):
    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])
    review = review.lower()
    review = review.split()
    review = [ps.stem(word) for word in review if word not in stopwords.words('english')]
    review = ' '.join(review)
    corpus_ps.append(review)

corpus_ps


corpus_ss = []

for i in range(0,len(messages)):
    review = re.sub('[^a-zA-Z]',' ',messages['message'][i])
    review = review.lower()
    review = review.split()
    review = [ss.stem(word) for word in review if word not in stopwords.words('english')]
    review = ' '.join(review)
    corpus_ss.append(review)

corpus_ss


corpus_lemma = []

for i in range(0,len(messages)):
    review = re.sub('',' ',messages['message'][i])
    review = review.lower()
    review = review.split()
    review = [lemmatizer.lemmatize(word) for word in review if word not in stopwords.words('english')]
    review = ' '.join(review)
    corpus_lemma.append(review)

corpus_lemma



